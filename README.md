# MiniGPT â€“ Character-Level GPT from Scratch

This project is a **mini GPT-style language model** implemented and trained **from scratch** using PyTorch.  
The model is trained on the **Tiny Shakespeare dataset** and generates Shakespeare-like text **character by character**

This project is inspired by understanding how GPT models work internally, rather than using pre-trained libraries.

---

## ðŸ”¹ Model Overview

- Decoder-only Transformer (GPT-style)
- Character-level language modeling
- Masked self-attention
- Multi-head attention
- Positional embeddings
- Trained from scratch (no pretraining)

---

## ðŸ”¹ Architecture Details

| Component | Value |
|--------|------|
| Embedding size | 64 |
| Number of heads | 4 |
| Number of layers | 4 |
| Context length | 32 |
| Vocabulary | Character-level |
| Optimizer | AdamW |
| Loss | Cross Entropy |

---

## ðŸ”¹ Training Details

- Dataset: Tiny Shakespeare
- Training split: 90%
- Validation split: 10%
- Total iterations: ~5000
- Hardware: Local machine (limited GPU memory)

The model was trained until both training and validation loss stabilized.

---

## ðŸ”¹ Training Progress

Below is a snapshot of the training phase showing both **training loss** and **validation loss** over iterations:

![Training Logs](assets\Screenshot 2026-01-13 175023.png)

The validation loss consistently decreases along with training loss, indicating stable learning without overfitting.

---

## ðŸ”¹ Sample Generated Output

Below is a sample text generated by the trained MiniGPT model:

![Generated Output](assets\Screenshot 2026-01-13 175050.png)

The model learns:
- Shakespeare-style dialogue formatting
- Character names
- Sentence structure and punctuation
- Play-like conversational flow

---

## ðŸ”¹ How to Run

### Install dependencies
```bash
pip install torch
